# -*- coding: utf-8 -*-
"""
å…³é”®è¯è°ƒåº¦å™¨æ¨¡å—
===============
è´Ÿè´£æœç´¢å…³é”®è¯ã€è§£ææœç´¢ç»“æœé¡µã€å‘ç°å¸–å­å¹¶å¹¶å‘è°ƒåº¦ PostScraperã€‚
"""

import asyncio
import random
from urllib.parse import urljoin
from typing import List

from lxml import etree

from ..config import CrawlerConfig
from ..http_client import HttpClient
from ..logger import get_logger
from ..storage import CsvStorage
from ..utils import extract_post_id_from_url
from .post_scraper import PostScraper

logger = get_logger("scrapers.keyword")

# å¾®åšæœç´¢é¡µé¢ URL
SEARCH_URL = "https://s.weibo.com/weibo"


class KeywordScraper:
    """
    å…³é”®è¯è°ƒåº¦å™¨ã€‚

    èŒè´£:
        - æœç´¢å…³é”®è¯çš„æ‰€æœ‰ç»“æœé¡µé¢
        - è§£æ HTML æå–å¸–å­åˆ—è¡¨
        - å»é‡è¿‡æ»¤å·²æŠ“å–å¸–å­
        - å¹¶å‘è°ƒåº¦ PostScraper å¤„ç†æ¯ä¸ªå¸–å­

    è¿™æ˜¯æŠ“å–æµç¨‹çš„æœ€é¡¶å±‚å…¥å£ã€‚
    """

    def __init__(
        self,
        client: HttpClient,
        post_scraper: PostScraper,
        storage: CsvStorage,
        config: CrawlerConfig,
    ) -> None:
        self._client = client
        self._post_scraper = post_scraper
        self._storage = storage
        self._config = config

    async def process_keyword(self, keyword: str) -> int:
        """
        å¤„ç†å•ä¸ªå…³é”®è¯çš„å…¨éƒ¨æœç´¢ç»“æœé¡µã€‚

        Args:
            keyword: æœç´¢å…³é”®è¯

        Returns:
            è¯¥å…³é”®è¯ä¸‹æŠ“å–çš„è¯„è®ºæ€»æ•°
        """
        logger.info("=" * 45)
        logger.info("ğŸ¯ æ­£åœ¨é‡‡é›†å…³é”®è¯ï¼šã€Œ%sã€", keyword)
        logger.info("=" * 45)

        max_posts = max(0, int(self._config.request.max_posts_per_keyword or 0))
        max_comments = max(0, int(self._config.request.max_comments_per_keyword or 0))
        existing_posts = self._storage.get_source_post_count("keyword", keyword)
        existing_comments = self._storage.get_keyword_count(keyword)

        if max_posts > 0 and existing_posts >= max_posts:
            logger.info(
                "â­ï¸ [%s] å·²è¾¾åˆ°å¸–å­ä¸Šé™ï¼š%d/%dï¼Œåœæ­¢è¯¥å…³é”®è¯",
                keyword,
                existing_posts,
                max_posts,
            )
            return 0

        if max_comments > 0 and existing_comments >= max_comments:
            logger.info(
                "â­ï¸ [%s] å·²è¾¾åˆ°è¯„è®ºä¸Šé™ï¼š%d/%dï¼Œåœæ­¢è¯¥å…³é”®è¯",
                keyword,
                existing_comments,
                max_comments,
            )
            return 0

        if existing_posts > 0 or existing_comments > 0:
            logger.info(
                "ğŸ“Š [%s] ç°æœ‰æ•°æ®: å¸–å­ %d%s | è¯„è®º %d%s",
                keyword,
                existing_posts,
                f"/{max_posts}" if max_posts > 0 else "",
                existing_comments,
                f"/{max_comments}" if max_comments > 0 else "",
            )

        total = 0
        page = 1
        max_pages = self._config.request.max_search_pages
        delay_range = self._config.delay.page_delay

        while page <= max_pages:
            current_posts = self._storage.get_source_post_count("keyword", keyword)
            current_comments = self._storage.get_source_count("keyword", keyword)

            if max_posts > 0 and current_posts >= max_posts:
                logger.info("ğŸ¯ [%s] å¸–å­è¾¾åˆ°ä¸Šé™ %dï¼Œåœæ­¢ç¿»é¡µ", keyword, max_posts)
                break
            if max_comments > 0 and current_comments >= max_comments:
                logger.info("ğŸ¯ [%s] è¯„è®ºè¾¾åˆ°ä¸Šé™ %dï¼Œåœæ­¢ç¿»é¡µ", keyword, max_comments)
                break

            remaining_posts = max_posts - current_posts if max_posts > 0 else 0
            try:
                page_total = await self._process_page(
                    keyword=keyword,
                    page=page,
                    remaining_posts=remaining_posts,
                )
                if page_total is None:
                    break  # æ— æ›´å¤šæ•°æ®æˆ–éœ€è¦ç™»å½•
                total += page_total
                page += 1
                await asyncio.sleep(random.uniform(*delay_range))

            except Exception as e:
                logger.warning("âš ï¸ [%s] ç¬¬ %d é¡µæŠ“å–å¼‚å¸¸: %s", keyword, page, e)
                await asyncio.sleep(2)
                page += 1

        logger.info(
            "[%s] å®Œæˆï¼Œæœ¬æ¬¡æ–°å¢è¯„è®º %d æ¡ï¼›å½“å‰ç´¯è®¡ å¸–å­ %d%s | è¯„è®º %d%s",
            keyword,
            total,
            self._storage.get_source_post_count("keyword", keyword),
            f"/{max_posts}" if max_posts > 0 else "",
            self._storage.get_source_count("keyword", keyword),
            f"/{max_comments}" if max_comments > 0 else "",
        )
        return total

    async def _process_page(
        self,
        keyword: str,
        page: int,
        remaining_posts: int,
    ) -> int | None:
        """
        å¤„ç†æœç´¢ç»“æœçš„å•ä¸ªé¡µé¢ã€‚

        Args:
            keyword: æœç´¢å…³é”®è¯
            page: é¡µç 

        Returns:
            æŠ“å–çš„è¯„è®ºæ•°ï¼ŒNone è¡¨ç¤ºåº”åœæ­¢ç¿»é¡µ
        """
        logger.info("ğŸ“„ [%s] æ‰«ææœç´¢ç»“æœç¬¬ %d é¡µ...", keyword, page)

        html_text = await self._client.get_html(
            SEARCH_URL,
            params={"q": keyword, "page": str(page)}
        )

        if not html_text:
            logger.warning("âš ï¸ [%s] ç¬¬ %d é¡µè¯·æ±‚å¤±è´¥ï¼Œè·³è¿‡", keyword, page)
            return 0

        # æ£€æµ‹ç™»å½•æ‹¦æˆª
        if "passport.weibo.com" in html_text:
            logger.error("âŒ Cookie å¤±æ•ˆæˆ–è¢«è¦æ±‚ç™»å½•ï¼åœæ­¢é‡‡é›†ã€‚")
            return None

        # è§£æå¸–å­åˆ—è¡¨
        cards = self._parse_feed_cards(html_text)
        if not cards:
            logger.info("ğŸ‘» [%s] ç¬¬ %d é¡µæ²¡æœ‰æ•°æ®äº†", keyword, page)
            return None

        page_post_limit = max(0, int(self._config.request.max_posts_per_search_page or 0))
        effective_limit = page_post_limit
        if remaining_posts > 0:
            if effective_limit <= 0:
                effective_limit = remaining_posts
            else:
                effective_limit = min(effective_limit, remaining_posts)

        # æå–æœ‰æ•ˆå¸–å­å¹¶å‘èµ·å¹¶å‘æŠ“å–
        post_tasks = self._collect_post_tasks(
            cards=cards,
            keyword=keyword,
            max_posts=effective_limit,
        )
        if not post_tasks:
            return 0

        # å¹¶å‘å¤„ç†æœ¬é¡µæ‰€æœ‰å¸–å­
        results = await asyncio.gather(*post_tasks, return_exceptions=True)
        total = 0
        for r in results:
            if isinstance(r, int):
                total += r
            elif isinstance(r, Exception):
                logger.warning("âš ï¸ å¸–å­å¹¶å‘å¤„ç†å¼‚å¸¸: %s", r)

        return total

    @staticmethod
    def _parse_feed_cards(html_text: str) -> list:
        """è§£æ HTML è·å–å¸–å­å¡ç‰‡å…ƒç´ åˆ—è¡¨"""
        html = etree.HTML(html_text)
        return html.xpath('//div[@action-type="feed_list_item"]')

    def _collect_post_tasks(
        self,
        cards: list,
        keyword: str,
        max_posts: int,
    ) -> List[asyncio.Task]:
        """
        ä»å¸–å­å¡ç‰‡ä¸­æå–æœ‰æ•ˆå¸–å­ï¼Œè¿”å›å¼‚æ­¥ä»»åŠ¡åˆ—è¡¨ã€‚

        Args:
            cards: HTML å¸–å­å¡ç‰‡å…ƒç´ 
            keyword: æœç´¢å…³é”®è¯

        Returns:
            å¾…æ‰§è¡Œçš„å¼‚æ­¥ä»»åŠ¡åˆ—è¡¨
        """
        tasks: List[asyncio.Task] = []
        seen_post_ids = set()

        for card in cards:
            post_id = self._extract_post_id(card)
            if not post_id:
                continue
            if post_id in seen_post_ids:
                continue
            seen_post_ids.add(post_id)
            tasks.append(
                self._post_scraper.process_post_id(
                    post_id=post_id,
                    source_mode="keyword",
                    source_target=keyword,
                )
            )
            if max_posts > 0 and len(tasks) >= max_posts:
                break

        return tasks

    @staticmethod
    def _extract_post_id(card: etree._Element) -> str:
        """
        ä»æœç´¢å¡ç‰‡ä¸­æå–å¸–å­ IDï¼ˆmid/id/bid å¤šæ ¼å¼å…¼å®¹ï¼‰ã€‚
        """
        mid = card.xpath("./@mid")
        if mid and mid[0]:
            return str(mid[0]).strip()

        hrefs = card.xpath(".//a[@href]/@href")
        for href in hrefs:
            full_url = href
            if href.startswith("//"):
                full_url = f"https:{href}"
            elif href.startswith("/"):
                full_url = urljoin("https://weibo.com", href)
            post_id = extract_post_id_from_url(full_url)
            if post_id:
                return post_id
        return ""
