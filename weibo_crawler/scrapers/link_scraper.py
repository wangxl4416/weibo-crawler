# -*- coding: utf-8 -*-
"""
é“¾æ¥æ¨¡å¼æŠ“å–å™¨
=============
æ”¯æŒæ‰¹é‡å¤„ç†å¾®åšå¸–å­é“¾æ¥ï¼Œè‡ªåŠ¨å…¼å®¹å¤šç§é“¾æ¥æ ¼å¼å¹¶æå–å¸–å­ IDã€‚
"""

import asyncio
from typing import List

from ..config import CrawlerConfig
from ..logger import get_logger
from ..utils import extract_post_id_from_url
from .post_scraper import PostScraper

logger = get_logger("scrapers.link")


class LinkScraper:
    """æŒ‰é“¾æ¥åˆ—è¡¨æŠ“å–å¾®åšå¸–å­æ•°æ®ã€‚"""

    def __init__(self, post_scraper: PostScraper, config: CrawlerConfig) -> None:
        self._post_scraper = post_scraper
        self._config = config
        self._semaphore = asyncio.Semaphore(config.concurrency.post_detail_concurrency)

    async def process_links(self, links: List[str], source_mode: str = "post_url") -> int:
        if not links:
            return 0

        tasks = [self._process_one(link, source_mode) for link in links if link.strip()]
        if not tasks:
            return 0

        results = await asyncio.gather(*tasks, return_exceptions=True)
        total_saved = 0
        for result in results:
            if isinstance(result, int):
                total_saved += result
            elif isinstance(result, Exception):
                logger.warning("é“¾æ¥æ¨¡å¼ä»»åŠ¡å¼‚å¸¸: %s", result)
        logger.info("ğŸ”— é“¾æ¥æ¨¡å¼å®Œæˆ: å¤„ç† %d æ¡é“¾æ¥ï¼Œæ–°å¢è¯„è®º %d æ¡", len(tasks), total_saved)
        return total_saved

    async def _process_one(self, link: str, source_mode: str) -> int:
        post_id = extract_post_id_from_url(link)
        if not post_id:
            logger.warning("âš ï¸ æ— æ³•ä»é“¾æ¥æå–å¸–å­ ID: %s", link)
            return 0

        async with self._semaphore:
            return await self._post_scraper.process_post_id(
                post_id=post_id,
                source_mode=source_mode,
                source_target=link,
            )
