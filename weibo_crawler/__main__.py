# -*- coding: utf-8 -*-
"""
åº”ç”¨ä¸»å…¥å£æ¨¡å—
=============
ç»Ÿä¸€è°ƒåº¦å¤šæŠ“å–æ¨¡å¼ï¼š
    - å…³é”®è¯æ¨¡å¼
    - é“¾æ¥æ¨¡å¼
    - ç”¨æˆ·æ¨¡å¼ï¼ˆä¸»é¡µ + æ—¶é—´çº¿ï¼‰
"""

import asyncio
import os
import time

from .config import CrawlerConfig, get_default_config
from .cookie_manager import WeiboCookieManager
from .http_client import HttpClient
from .logger import get_logger, setup_logging
from .scrapers.comment_scraper import CommentScraper
from .scrapers.keyword_scraper import KeywordScraper
from .scrapers.link_scraper import LinkScraper
from .scrapers.post_scraper import PostScraper
from .scrapers.user_scraper import UserScraper
from .storage import CsvStorage

logger = get_logger("app")


class CrawlerApp:
    """å¾®åšçˆ¬è™«ä¸»åº”ç”¨ï¼ˆå¤šæ¨¡å¼ï¼‰ã€‚"""

    def __init__(self, config: CrawlerConfig | None = None) -> None:
        self._config = config or get_default_config()
        self._storage = CsvStorage(self._config)

    async def run(self) -> None:
        setup_logging()
        self._storage.load_history()

        cookie_manager = WeiboCookieManager()
        self._config.cookies = await cookie_manager.get_cookies()

        cc = self._config.concurrency
        keyword_targets = self._config.active_keyword_targets
        post_url_targets = self._config.active_post_url_targets
        user_targets = self._config.active_user_targets

        logger.info("ğŸš€ å¼€å§‹å¾®åšå¤šæ¨¡å¼é‡‡é›†")
        logger.info(
            "âš¡ å¹¶å‘é…ç½®: å…³é”®è¯Ã—%d | å¸–å­è¯¦æƒ…Ã—%d | è¯„è®ºÃ—%d | ç”¨æˆ·Ã—%d | å…¨å±€è¯·æ±‚Ã—%d",
            cc.keyword_concurrency,
            cc.post_detail_concurrency,
            cc.comment_concurrency,
            cc.user_concurrency,
            cc.global_concurrency,
        )
        logger.info(
            "ğŸ¯ ç›®æ ‡ç»Ÿè®¡: å…³é”®è¯ %d | å¸–å­é“¾æ¥ %d | ç”¨æˆ·ç›®æ ‡ %d",
            len(keyword_targets),
            len(post_url_targets),
            len(user_targets),
        )
        logger.info(
            "ğŸ§© è¿è¡Œæ¨¡å¼: %s | ä¿å­˜æ ¼å¼: %s",
            ",".join(self._config.enabled_modes),
            self._config.normalized_save_format,
        )
        logger.info(
            "ğŸ’¬ è¯„è®ºç­–ç•¥: keyword=%s post_url=%s user=%s | ä¸€çº§=%s äºŒçº§=%s",
            self._config.comments.enable_for_keyword,
            self._config.comments.enable_for_post_url,
            self._config.comments.enable_for_user,
            self._config.comments.fetch_top_level,
            self._config.comments.fetch_sub_level,
        )
        logger.info(
            "ğŸ–¼ï¸ åª’ä½“ç­–ç•¥: mode=%s | ç±»å‹=%s | ä¸»é¡µåª’ä½“=%s | ä¸‹è½½=%s | è¦†ç›–=%s",
            self._config.media_mode_enabled,
            self._config.media_type_mode_enabled,
            self._config.profile_media_enabled,
            self._config.download.enable_media_download,
            self._config.download.overwrite_existing,
        )
        logger.info("ğŸ‘¤ ä¸»é¡µä¿¡æ¯æŠ“å–: %s", self._config.profile_info_enabled)
        logger.info(
            "ğŸ“ æŠ“å–ä¸Šé™: æ¯é¡µå¸–å­ %d | æ¯å¸–è¯„è®º %d | æ¯å…³é”®è¯å¸–å­ %d | æ¯å…³é”®è¯è¯„è®º %d | æ¯ç”¨æˆ·ç¿»é¡µ %s | æ¯ç”¨æˆ·å¸–å­ %d",
            self._config.request.max_posts_per_search_page,
            self._config.request.max_comments_per_post,
            self._config.request.max_posts_per_keyword,
            self._config.request.max_comments_per_keyword,
            str(self._config.max_user_pages_enabled) if self._config.max_user_pages_enabled > 0 else "ä¸é™",
            self._config.request.max_posts_per_user,
        )

        await self._storage.start_writer()
        try:
            async with HttpClient(self._config) as client:
                comment_scraper = CommentScraper(client, self._config)
                post_scraper = PostScraper(
                    client=client,
                    comment_scraper=comment_scraper,
                    storage=self._storage,
                    config=self._config,
                )
                keyword_scraper = KeywordScraper(
                    client=client,
                    post_scraper=post_scraper,
                    storage=self._storage,
                    config=self._config,
                )
                link_scraper = LinkScraper(post_scraper=post_scraper, config=self._config)
                user_scraper = UserScraper(
                    client=client,
                    post_scraper=post_scraper,
                    storage=self._storage,
                    config=self._config,
                )

                if keyword_targets:
                    await self._run_keyword_mode(keyword_scraper)

                if self._config.is_mode_enabled("post_url") and post_url_targets:
                    await link_scraper.process_links(
                        post_url_targets,
                        source_mode="post_url",
                    )
                elif self._config.is_mode_enabled("post_url"):
                    logger.warning("âš ï¸ é“¾æ¥æ¨¡å¼æœªè·å–åˆ°å¯ç”¨ç›®æ ‡ï¼Œå·²è·³è¿‡")

                if self._config.is_mode_enabled("user") and user_targets:
                    await user_scraper.process_users(
                        user_targets,
                        force_fetch_timeline=True,
                        force_fetch_profile=self._config.profile_info_enabled,
                    )
                elif self._config.is_mode_enabled("user"):
                    logger.warning("âš ï¸ ç”¨æˆ·æ¨¡å¼æœªè·å–åˆ°å¯ç”¨ç›®æ ‡ï¼Œå·²è·³è¿‡")

        except KeyboardInterrupt:
            logger.warning("ğŸ›‘ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as exc:
            logger.error("âŒ å‘ç”Ÿå¼‚å¸¸: %s", exc, exc_info=True)
        finally:
            await self._storage.stop_writer()
            self._log_final_summary()

    async def _run_keyword_mode(self, keyword_scraper: KeywordScraper) -> None:
        keywords = self._config.active_keyword_targets
        if not keywords:
            logger.warning("âš ï¸ keyword æ¨¡å¼å·²å¯ç”¨ï¼Œä½† TARGET_KEYWORDS ä¸ºç©ºï¼Œå·²è·³è¿‡")
            return
        batch_size = max(1, self._config.concurrency.keyword_concurrency)

        logger.info("")
        logger.info("ğŸ” å¯åŠ¨å…³é”®è¯æ¨¡å¼")
        for i in range(0, len(keywords), batch_size):
            batch = keywords[i: i + batch_size]
            batch_num = i // batch_size + 1
            logger.info("ğŸ“¦ å…³é”®è¯æ‰¹æ¬¡ %d: %s", batch_num, ", ".join(batch))
            tasks = [keyword_scraper.process_keyword(keyword) for keyword in batch]
            await asyncio.gather(*tasks, return_exceptions=True)
            logger.info("ğŸ“Š å½“å‰ç´¯è®¡è¯„è®º: %d", self._storage.total_comments_saved)

    def _log_final_summary(self) -> None:
        logger.info("")
        logger.info(
            "ğŸ‰ é‡‡é›†å®Œæˆ: è¯„è®º %d | å¸–å­ %d | åª’ä½“ %d | ä¸»é¡µ %d",
            self._storage.total_comments_saved,
            self._storage.total_posts_saved,
            self._storage.total_media_saved,
            self._storage.total_profiles_saved,
        )
        for name, file_path in self._storage.output_summary.items():
            logger.info("ğŸ“‚ %-9s -> %s", name, os.path.abspath(file_path))


def main() -> None:
    start_time = time.time()
    app = CrawlerApp()
    asyncio.run(app.run())
    elapsed = time.time() - start_time
    logger.info("â± æ€»è€—æ—¶: %.1f ç§’ (%.1f åˆ†é’Ÿ)", elapsed, elapsed / 60)


if __name__ == "__main__":
    main()
